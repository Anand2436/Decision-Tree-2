{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "# Making arrays, input(x) and output(y) for or gate\n",
    "# x1 x2 y\n",
    "# 0  0  0\n",
    "# 0  1  1\n",
    "# 1  0  1\n",
    "# 1  1  1\n",
    "\n",
    "# Therefore x array is 2d array(4*2) and y array is 1d array(4,1)\n",
    "\n",
    "x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "print(x.shape)\n",
    "\n",
    "y = np.array([0,1,1,1])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calcuting entropy for a particular node with total nodes , class A nodes and class B nodes passed as arguments\n",
    "# import math for calculating log\n",
    "\n",
    "import math\n",
    "\n",
    "def entropy(totalSamples,aSamples,bSamples):\n",
    "    if aSamples == totalSamples: # class A pure node\n",
    "        return 0.0\n",
    "    elif bSamples == totalSamples: # class B pure node\n",
    "        return 0.0\n",
    "    else :\n",
    "        pA = aSamples / totalSamples\n",
    "        pB = bSamples / totalSamples\n",
    "        log_pA = math.log(pA,2)\n",
    "        log_pB = math.log(pB,2)\n",
    "        \n",
    "        # Entropy or info_Node\n",
    "        info_Node = - (pA * log_pA) - (pB * log_pB)\n",
    "        return info_Node\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8112781244591328\n"
     ]
    }
   ],
   "source": [
    "c = entropy(4,3,1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def current_entropy(x,y):\n",
    "        totalSamples = len(y)\n",
    "        classA = 0\n",
    "        classB = 0\n",
    "        for i in range(totalSamples):\n",
    "            if y[i]==1:\n",
    "                classA += 1\n",
    "            else :\n",
    "                classB += 1\n",
    "            \n",
    "        gain_Current_Node = entropy(totalSamples,classA,classB)\n",
    "        return gain_Current_Node,classA,classB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will be using same function for entropy as well as split_info. Just arguments will be different.\n",
    " \n",
    " 1.rightSamples : samples going to the right node \n",
    " \n",
    " 2.leftSamples : samples going to the left node\n",
    "\n",
    " 3.So, if we pass rightSamples and leftSamples in entropy function instead of classes(aSample, bSample) sample, we will get split_info instead of entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining gain ration function with three arguments , x (input), y(output), feat ind : feature (index) under consideration, entropy of current node\n",
    "\n",
    "def gain_ratio(x,y,feat_ind,gain_Current_Node):\n",
    "    # calculate gain for current node before splitting\n",
    "    totalSamples = len(y)\n",
    "    \n",
    "    rightSamples = 0 # count of samples going in right node \n",
    "    leftSamples = 0 # count of samples going in left node\n",
    "    # the above variables needed to calculate split info\n",
    "    \n",
    "    cr1 = 0 # count of 1s in right node where x[feat_ind][] will be false\n",
    "    cr0 = 0 # count of 0s i right node where x[feat_ind][] will be false\n",
    "    # the above two will be needed to calculate entropy for right node\n",
    "    \n",
    "    cl1 = 0 # count of 1s in left node where x[feat_ind][] will be true\n",
    "    cl0 = 0 # count of 0s in left node where x[feat_ind][] will be true\n",
    "    # the above two will be needed to calculate entropy for left node\n",
    "    \n",
    "    for j in range(totalSamples):\n",
    "        if x[j][feat_ind] == 1:\n",
    "            leftSamples = leftSamples + 1;\n",
    "            if y[j] == 1:\n",
    "                cl1 = cl1 + 1\n",
    "            else:\n",
    "                cl0 = cl0 + 1\n",
    "        else :\n",
    "            rightSamples = rightSamples + 1\n",
    "            if y[j] == 1:\n",
    "                cr1 = cr1 + 1\n",
    "            else :\n",
    "                cr0 = cr0 + 1\n",
    "    \n",
    "    # Entropy of right subNode : entRight\n",
    "    # Entropy of left subNode : entLeft\n",
    "    entRight = entropy(rightSamples,cr1,cr0)\n",
    "    entLeft = entropy(leftSamples,cl1,cl0)\n",
    "    \n",
    "    # Info Gain(entropy) of split nodes without considering split info [using weighted addition]\n",
    "    gain_Split_Nodes = ((rightSamples/totalSamples) * entRight)+ ((leftSamples/totalSamples) * entLeft);\n",
    "    \n",
    "    # gain = original_node_entropy - split_nodes_entropy\n",
    "    gain = gain_Current_Node - gain_Split_Nodes;\n",
    "    \n",
    "    # Calculating split info for consideration in gain_ratio formula\n",
    "    split_info = entropy(totalSamples,rightSamples,leftSamples)\n",
    "    \n",
    "    if split_info != 0:\n",
    "        gain_ratio = gain / split_info\n",
    "    else :\n",
    "        gain_ratio = gain\n",
    "        \n",
    "    return gain_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31127812445913283\n",
      "0.31127812445913283\n"
     ]
    }
   ],
   "source": [
    "# checking gain ratio function\n",
    "# ans 1 for choosing x0 \n",
    "# ans 1 is a pair consisting of (gain ratio and current entropy)\n",
    "gain_Current = current_entropy(x,y);\n",
    "\n",
    "ans1 = gain_ratio(x,y,0,gain_Current[0])\n",
    "print(ans1)\n",
    "ans2 = gain_ratio(x,y,1,gain_Current[0])\n",
    "print(ans2)\n",
    "\n",
    "# both are same for or gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our recursive decision tree\n",
    "# arguments passed are \n",
    "# 1) x for input\n",
    "# 2) y for output\n",
    "# 3) counterLevel stating level of decision tree, we have reached\n",
    "# 4) features_added : telling which features have been added for printing right index \n",
    "\n",
    "def recursiveDT(x,y,counterLevel,features_added):\n",
    "    total_features = x.shape[1]\n",
    "    \n",
    "    max_gain = -527534723572\n",
    "    ans_feature = -1\n",
    "    \n",
    "    gain_Current = current_entropy(x,y)\n",
    "    \n",
    "    \n",
    "    # Calculating which feature to split on\n",
    "    for i in range(total_features):\n",
    "        ans1 = gain_ratio(x,y,i,gain_Current[0])\n",
    "        \n",
    "        if ans1 > max_gain:\n",
    "            max_gain = ans1\n",
    "            ans_feature = i\n",
    "            \n",
    "    # if a feature found\n",
    "    if ans_feature !=-1:\n",
    "        print(\"Level \",counterLevel)\n",
    "        counterLevel += 1\n",
    "        \n",
    "        if(gain_Current[1]!=0):\n",
    "            print(\"Number of Ones(True) : \",gain_Current[1])\n",
    "        if(gain_Current[2]!=0):\n",
    "            print(\"Number of Zeros(False) : \",gain_Current[2])\n",
    "        \n",
    "        print(\"Current Entropy is : \",gain_Current[0])\n",
    "        \n",
    "        # if entropy not equal to zero\n",
    "        if(gain_Current[0]!=0):\n",
    "            if ans_feature in features_added:\n",
    "                print(\"Splitting on feature X\",ans_feature + 1,\" with gain ratio \")\n",
    "            else:\n",
    "                print(\"Splitting on feature X\",ans_feature,\" with gain ratio \")\n",
    "                features_added[ans_feature] = True\n",
    "            print(max_gain)\n",
    "        else: # incase entropy equal to zero , means leaf node\n",
    "            print(\"Reached Leaf Node.\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        #First creating empty lists to breakdown our inputs according to the feature decided\n",
    "        \n",
    "        x1 = [[]]\n",
    "        x2 = [[]]\n",
    "        y1 =[]\n",
    "        y2 = []\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            var = np.ndarray.tolist(x[i]) # converting array to list\n",
    "            del var[ans_feature] # removing the feature enteries,that we have already calculated\n",
    "            if x[i][ans_feature]==1:\n",
    "                x1.append(var)\n",
    "                y1.append(y[i])\n",
    "            else :\n",
    "                x2.append(var)\n",
    "                y2.append(y[i])\n",
    "    \n",
    "    # converting the lists to arrays\n",
    "        x1.remove([])\n",
    "        x2.remove([])\n",
    "    \n",
    "        x1 = np.array(x1)\n",
    "        x2 = np.array(x2)\n",
    "        y1 = np.array(y1)\n",
    "        y2 = np.array(y2)\n",
    "        \n",
    "        if(gain_Current[0]!=0): # call recursively if current entropy not equal to zero\n",
    "            recursiveDT(x2,y2,counterLevel,features_added)\n",
    "            recursiveDT(x1,y1,counterLevel,features_added)\n",
    "    \n",
    "        return\n",
    "    \n",
    "    else: # if No feature to split upon\n",
    "        \n",
    "        print(\"Level \",counterLevel)\n",
    "        counterLevel += 1\n",
    "        \n",
    "        if gain_Current[1]!=0:\n",
    "            print(\"Number of Ones(True) : \",gain_Current[1])\n",
    "        if gain_Current[2]!=0:\n",
    "            print(\"Number of Zeros(False) : \",gain_Current[2])\n",
    "        \n",
    "        print(\"Current Entropy is : \",gain_Current[0])\n",
    "        \n",
    "        # if entropy not equal to zero\n",
    "        if(gain_Current[0]!=0):\n",
    "            if ans_feature in features_added:\n",
    "                print(\"Splitting on feature X\",ans_feature + 1,\" with gain ratio \")\n",
    "            else:\n",
    "                print(\"Splitting on feature X\",ans_feature,\" with gain ratio \")\n",
    "                features_added[ans_feature] = True\n",
    "            print(max_gain)\n",
    "            \n",
    "        else: # if entropy is zero that means we are at a leaf node\n",
    "            print(\"Reached Leaf Node.\")\n",
    "        print()\n",
    "        return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level  0\n",
      "Number of Ones(True) :  3\n",
      "Number of Zeros(False) :  1\n",
      "Current Entropy is :  0.8112781244591328\n",
      "Splitting on feature X 0  with gain ratio \n",
      "0.31127812445913283\n",
      "\n",
      "Level  1\n",
      "Number of Ones(True) :  1\n",
      "Number of Zeros(False) :  1\n",
      "Current Entropy is :  1.0\n",
      "Splitting on feature X 1  with gain ratio \n",
      "1.0\n",
      "\n",
      "Level  2\n",
      "Number of Zeros(False) :  1\n",
      "Current Entropy is :  0.0\n",
      "Reached Leaf Node.\n",
      "\n",
      "Level  2\n",
      "Number of Ones(True) :  1\n",
      "Current Entropy is :  0.0\n",
      "Reached Leaf Node.\n",
      "\n",
      "Level  1\n",
      "Number of Ones(True) :  2\n",
      "Current Entropy is :  0.0\n",
      "Reached Leaf Node.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_added = {} # to keep which features are added for recursive calls\n",
    "# idea is that if a feature in a recursive call is already in features_added dictionary then the index of feature will be\n",
    "# ans_feature = ans_feature + 1\n",
    "\n",
    "\n",
    "recursiveDT(x,y,0,features_added)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
